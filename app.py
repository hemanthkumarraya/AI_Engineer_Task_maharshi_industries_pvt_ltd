import streamlit as st
import torch
import cv2
import numpy as np
from torchvision.models import resnet18
from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision import transforms
from PIL import Image
from paddleocr import PaddleOCR
import tempfile
import os
import json
# Page Config
st.set_page_config(page_title="Industrial AI System", layout="wide")
st.title("ðŸ› ï¸ Industrial AI Vision System")
st.markdown("**Offline Computer Vision Pipeline** â€“ Detection, Classification & OCR")
# Sidebar: Mode Selection
mode = st.sidebar.selectbox(
Â Â Â Â "Choose Task",
Â Â Â Â ["Object Detection + Human/Animal Classification", "Offline Industrial OCR"]
)
# Common Settings
st.sidebar.header("âš™ï¸ Common Settings")
conf_threshold = st.sidebar.slider("Confidence Threshold", 0.1, 0.95, 0.5, 0.05)
# Model Paths
DETECTOR_PATH = "faster_rcnn_object_detector_FAST.pth"
CLASSIFIER_PATH = "animal_classifier.pth"
# Load Models (cached)
@st.cache_resource
def load_detector():
Â Â Â Â model = fasterrcnn_mobilenet_v3_large_320_fpn(weights=None, num_classes=2)
Â Â Â Â in_features = model.roi_heads.box_predictor.cls_score.in_features
Â Â Â Â model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=2)
Â Â Â Â model.load_state_dict(torch.load(DETECTOR_PATH, map_location="cpu"))
Â Â Â Â model.eval()
Â Â Â Â return model
@st.cache_resource
def load_classifier():
Â Â Â Â model = resnet18(weights=None)
Â Â Â Â model.fc = torch.nn.Linear(model.fc.in_features, 2)
Â Â Â Â model.load_state_dict(torch.load(CLASSIFIER_PATH, map_location="cpu"))
Â Â Â Â model.eval()
Â Â Â Â return model
@st.cache_resource
def load_ocr():
Â Â Â Â return PaddleOCR(use_angle_cls=True, lang='en', use_gpu=False, show_log=False)
# Transforms
classify_transform = transforms.Compose([
Â Â Â Â transforms.Resize((224, 224)),
Â Â Â Â transforms.ToTensor(),
Â Â Â Â transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
# ================ PART A: Detection + Classification ================
if mode == "Object Detection + Human/Animal Classification":
Â Â Â Â st.header("ðŸ” Object Detection + Human/Animal Classification")
Â Â Â Â st.markdown("Detects living objects â†’ Classifies as **Human** (Green) or **Animal** (Red)")
Â Â Â Â detector = load_detector()
Â Â Â Â classifier = load_classifier()
Â Â Â Â def process_image_part_a(image_bgr, threshold):
Â Â Â Â Â Â Â Â img_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
Â Â Â Â Â Â Â Â img_tensor = transforms.ToTensor()(img_rgb).unsqueeze(0)
Â Â Â Â Â Â Â Â with torch.no_grad():
Â Â Â Â Â Â Â Â Â Â Â Â predictions = detector(img_tensor)[0]
Â Â Â Â Â Â Â Â boxes = predictions['boxes'].cpu().numpy()
Â Â Â Â Â Â Â Â scores = predictions['scores'].cpu().numpy()
Â Â Â Â Â Â Â Â output_img = image_bgr.copy()
Â Â Â Â Â Â Â Â crops = []
Â Â Â Â Â Â Â Â results = []
Â Â Â Â Â Â Â Â for box, score in zip(boxes[scores > threshold], scores[scores > threshold]):
Â Â Â Â Â Â Â Â Â Â Â Â x1, y1, x2, y2 = map(int, box)
Â Â Â Â Â Â Â Â Â Â Â Â crop = img_rgb[y1:y2, x1:x2]
Â Â Â Â Â Â Â Â Â Â Â Â if crop.size == 0: continue
Â Â Â Â Â Â Â Â Â Â Â Â # Classification
Â Â Â Â Â Â Â Â Â Â Â Â crop_pil = Image.fromarray(crop)
Â Â Â Â Â Â Â Â Â Â Â Â input_tensor = classify_transform(crop_pil).unsqueeze(0)
Â Â Â Â Â Â Â Â Â Â Â Â with torch.no_grad():
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â output = classifier(input_tensor)
Â Â Â Â Â Â Â Â Â Â Â Â pred_idx = output.argmax(1).item()
Â Â Â Â Â Â Â Â Â Â Â Â label = "Animal" if pred_idx == 0 else "Human"
Â Â Â Â Â Â Â Â Â Â Â Â conf = torch.softmax(output, dim=1)[0][pred_idx].item()
Â Â Â Â Â Â Â Â Â Â Â Â crops.append(crop)
Â Â Â Â Â Â Â Â Â Â Â Â results.append((label, conf))
Â Â Â Â Â Â Â Â Â Â Â Â # Draw on output
Â Â Â Â Â Â Â Â Â Â Â Â color = (0, 0, 255) if label == "Animal" else (0, 255, 0)
Â Â Â Â Â Â Â Â Â Â Â Â cv2.rectangle(output_img, (x1, y1), (x2, y2), color, 3)
Â Â Â Â Â Â Â Â Â Â Â Â cv2.putText(output_img, f"{label} ({conf:.2f})", (x1, y1 - 10),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 3)
Â Â Â Â Â Â Â Â return output_img, crops, results
Â Â Â Â uploaded = st.file_uploader("Upload Image/Video", type=["jpg", "jpeg", "png", "mp4"], key="parta")
Â Â Â Â if uploaded:
Â Â Â Â Â Â Â Â if uploaded.type.startswith('image'):
Â Â Â Â Â Â Â Â Â Â Â Â image = np.array(Image.open(uploaded))
Â Â Â Â Â Â Â Â Â Â Â Â image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
Â Â Â Â Â Â Â Â Â Â Â Â result_img, crops, results = process_image_part_a(image_bgr, conf_threshold)
Â Â Â Â Â Â Â Â Â Â Â Â st.image(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB), use_column_width=True)
Â Â Â Â Â Â Â Â Â Â Â Â if crops:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.subheader("Cropped Classifications")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cols = st.columns(min(len(crops), 4))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â for i, (crop, (label, conf)) in enumerate(zip(crops, results)):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â with cols[i % 4]:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.image(crop, caption=f"{label} ({conf:.2f})")
Â Â Â Â Â Â Â Â else: # Video
Â Â Â Â Â Â Â Â Â Â Â Â st.video(uploaded)
Â Â Â Â Â Â Â Â Â Â Â Â if st.button("Process Video"):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â temp_in = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4").name
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â with open(temp_in, "wb") as f:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â f.write(uploaded.getbuffer())
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cap = cv2.VideoCapture(temp_in)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â fps = cap.get(cv2.CAP_PROP_FPS)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â temp_out = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4").name
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â writer = cv2.VideoWriter(temp_out, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â progress = st.progress(0)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â frame_count = 0
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â while cap.isOpened():
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ret, frame = cap.read()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â if not ret: break
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â processed, _, _ = process_image_part_a(frame, conf_threshold)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â writer.write(processed)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â frame_count += 1
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â progress.progress(frame_count / total)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cap.release()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â writer.release()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â progress.empty()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.video(temp_out)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â with open(temp_out, "rb") as f:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.download_button("Download Annotated Video", f, "detection_video.mp4")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â os.unlink(temp_in)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â os.unlink(temp_out)
# ================ PART B: Industrial OCR ================
else:
Â Â Â Â st.header("ðŸ“„ Offline Industrial OCR")
Â Â Â Â st.markdown("Extracts **faded/stenciled text** from industrial boxes (military crates, serial numbers, etc.)")
Â Â Â Â ocr = load_ocr()
Â Â Â Â def ocr_preprocess(img):
Â Â Â Â Â Â Â Â gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
Â Â Â Â Â Â Â Â clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8,8))
Â Â Â Â Â Â Â Â enhanced = clahe.apply(gray)
Â Â Â Â Â Â Â Â denoised = cv2.fastNlMeansDenoising(enhanced, h=15)
Â Â Â Â Â Â Â Â binary = cv2.adaptiveThreshold(denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 15, 5)
Â Â Â Â Â Â Â Â return cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)
Â Â Â Â uploaded = st.file_uploader("Upload Image/Video", type=["jpg", "jpeg", "png", "mp4"], key="ocr")
Â Â Â Â if uploaded:
Â Â Â Â Â Â Â Â if uploaded.type.startswith('image'):
Â Â Â Â Â Â Â Â Â Â Â Â col1, col2 = st.columns(2)
Â Â Â Â Â Â Â Â Â Â Â Â with col1:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.subheader("Original")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â img_pil = Image.open(uploaded)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.image(img_pil, use_column_width=True)
Â Â Â Â Â Â Â Â Â Â Â Â img_cv = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
Â Â Â Â Â Â Â Â Â Â Â Â preprocessed = ocr_preprocess(img_cv)
Â Â Â Â Â Â Â Â Â Â Â Â with col2:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.subheader("Preprocessed (for OCR)")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.image(cv2.cvtColor(preprocessed, cv2.COLOR_BGR2RGB), use_column_width=True)
Â Â Â Â Â Â Â Â Â Â Â Â with st.spinner("Extracting text..."):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â result = ocr.ocr(preprocessed, cls=True)
Â Â Â Â Â Â Â Â Â Â Â Â output_img = img_cv.copy()
Â Â Â Â Â Â Â Â Â Â Â Â extracted = []
Â Â Â Â Â Â Â Â Â Â Â Â for line in result or []:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â for word in line:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â bbox = np.array(word[0], np.int32)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â text, conf = word[1]
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â if conf > conf_threshold:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cv2.polylines(output_img, [bbox], True, (0, 255, 0), 3)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cv2.putText(output_img, f"{text} ({conf:.2f})", (bbox[0][0], bbox[0][1]-10),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â extracted.append({"text": text, "confidence": round(conf, 3)})
Â Â Â Â Â Â Â Â Â Â Â Â st.subheader("OCR Result")
Â Â Â Â Â Â Â Â Â Â Â Â st.image(cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB), use_column_width=True)
Â Â Â Â Â Â Â Â Â Â Â Â st.subheader("Extracted Text")
Â Â Â Â Â Â Â Â Â Â Â Â if extracted:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.json(extracted)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.download_button("Download JSON", json.dumps(extracted, indent=2), "ocr_results.json")
Â Â Â Â Â Â Â Â Â Â Â Â else:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.info("No text detected above threshold.")
Â Â Â Â Â Â Â Â else: # Video OCR (every 10th frame)
Â Â Â Â Â Â Â Â Â Â Â Â st.video(uploaded)
Â Â Â Â Â Â Â Â Â Â Â Â if st.button("Process Video (OCR)"):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â temp_in = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4").name
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â with open(temp_in, "wb") as f:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â f.write(uploaded.getbuffer())
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cap = cv2.VideoCapture(temp_in)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â fps = cap.get(cv2.CAP_PROP_FPS)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â w, h = int(cap.get(3)), int(cap.get(4))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â temp_out = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4").name
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â writer = cv2.VideoWriter(temp_out, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â progress = st.progress(0)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â frame_count = 0
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â while cap.isOpened():
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ret, frame = cap.read()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â if not ret: break
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â if frame_count % 10 == 0:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â pre = ocr_preprocess(frame)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â res = ocr.ocr(pre, cls=True)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â for line in res or []:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â for word in line:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â if word[1][1] > conf_threshold:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â bbox = np.array(word[0], np.int32)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cv2.polylines(frame, [bbox], True, (0, 255, 255), 2)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cv2.putText(frame, word[1][0], (bbox[0][0], bbox[0][1]-10),
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â writer.write(frame)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â frame_count += 1
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â progress.progress(frame_count / total)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â cap.release()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â writer.release()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â progress.empty()
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.video(temp_out)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â with open(temp_out, "rb") as f:
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â st.download_button("Download OCR Video", f, "ocr_video.mp4")
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â os.unlink(temp_in)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â os.unlink(temp_out)
st.caption("Fully offline â€¢ Custom-trained models â€¢ No cloud APIs")
